{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "1. E3-substrates-interactions file is downloaded from : http://ubibrowser.bio-it.cn/ubibrowser_v3/home/download\n",
    "2. Collected_degrons file is downloaded from: http://degron.phasep.pro/download/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Convert E3-substrates-interactions text file to Excel file'''\n",
    "# read in the text file using pandas\n",
    "df = pd.read_csv('../datasets/E3-substrates-interactions.txt', delimiter='\\t')\n",
    "\n",
    "# write the dataframe to an Excel file\n",
    "df.to_excel('../datasets/E3-substrates-interactions.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "1. Read the datasets\n",
    "2. Remove the rows with nan or invalid IDs on Entry column in degrons, and unreviewd on their Status column\n",
    "3. Remove the rows with nan or invalid IDs on both SwissProt AC (Substrate) and SwissProt AC (E3) columns\n",
    "4. Merge them together on Entry to a final dataset\n",
    "5. Keep needed columns: SwissProt AC (Substrate), SwissProt AC (E3), protein seq, start, end, deg_seq, E3_sequence\n",
    "6. Explode the rows containing # in their E3\n",
    "7. Add E3 sequences to the dataframe\n",
    "8. Groupby the final dataset to merge rows with the same substrate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the xlsx files using pandas\n",
    "degrons = pd.read_excel('../datasets/collected_degrons.xlsx')\n",
    "interactions = pd.read_excel('../datasets/E3-substrates-interactions.xlsx')\n",
    "E3_sequences = pd.read_excel('../datasets/uniprot-E3.xlsx')\n",
    "\n",
    "# Remove rows from both dataframes that have NaN or invalid ids\n",
    "degrons = degrons[(degrons['Entry'].notnull()) & (degrons['Entry'] != '-')]\n",
    "degrons = degrons[degrons['Status'] != 'unreviewd']\n",
    "\n",
    "# Remove the rows with nan or invalid IDs on both SwissProt AC (Substrate) and SwissProt AC (E3) columns\n",
    "interactions = interactions[(interactions['SwissProt AC (Substrate)'].notnull()) & (interactions['SwissProt AC (Substrate)'] != '-')]\n",
    "interactions = interactions[(interactions['SwissProt AC (E3)'].notnull()) & (interactions['SwissProt AC (E3)'] != '-')]\n",
    "\n",
    "# Merge them together on Entry to a final dataset\n",
    "final = pd.merge(degrons, interactions, left_on='Entry', right_on='SwissProt AC (Substrate)')\n",
    "final = pd.merge(final, E3_sequences, left_on='SwissProt AC (E3)', right_on='Entry')\n",
    "\n",
    "# Keep needed columns: SwissProt AC (Substrate), SwissProt AC (E3), protein seq, start, end, deg_seq\n",
    "final = final[['SwissProt AC (Substrate)', 'SwissProt AC (E3)', 'protein seq', 'start', 'end', 'deg_seq', 'E3_sequence']]\n",
    "final.columns = ['substrate', 'E3', 'substrate protein seq', 'start', 'end', 'degron_seq', 'E3_sequence']\n",
    "final = final.assign(E3=final['E3'].str.split('#')).explode('E3')\n",
    "final = final.drop_duplicates(subset=['substrate', 'E3'], keep='first')\n",
    "\n",
    "# Creat a map to store degrons and their corresponding substrates\n",
    "degron_substrate = {}\n",
    "for index, row in final.iterrows():\n",
    "    if row['degron_seq'] not in degron_substrate:\n",
    "        degron_substrate[row['degron_seq']] = row['substrate']\n",
    "        \n",
    "#Save the degron_substrate dictionary to a csv file\n",
    "with open('../datasets/degron_substrate.csv', 'w') as f:\n",
    "    for key in degron_substrate.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,degron_substrate[key]))\n",
    "\n",
    "# Creat a dictionary to store the E3s and their corresponding E3 sequences\n",
    "E3_sequence = {}\n",
    "for index, row in final.iterrows():\n",
    "    if row['E3'] not in E3_sequence:\n",
    "        E3_sequence[row['E3']] = row['E3_sequence']\n",
    "\n",
    "# Save the E3_sequence dictionary to a csv file\n",
    "with open('../datasets/E3_sequence.csv', 'w') as f:\n",
    "    for key in E3_sequence.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,E3_sequence[key]))\n",
    "\n",
    "# Groupby the final dataset to merge rows with the same substrate \n",
    "#final = final.groupby(['substrate', 'substrate protein seq', 'start', 'end', 'degron_seq']).agg({'E3': lambda x: ';'.join(x), 'E3_sequence': lambda x: ';'.join(x)}).reset_index()\n",
    "\n",
    "# Write the final dataset to an Excel file\n",
    "final.to_excel('../datasets/final_dataset.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395, 4)\n",
      "(99, 4)\n"
     ]
    }
   ],
   "source": [
    "# Test and train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(final, test_size=0.2, random_state=42)\n",
    "\n",
    "# Remove substrate protein seq, start, end columns from both train and test datasets\n",
    "train = train.drop(columns=['substrate protein seq', 'start', 'end'])\n",
    "test = test.drop(columns=['substrate protein seq', 'start', 'end'])\n",
    "\n",
    "#Print the size of the train and test datasets\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "# Write the train and test datasets to Excel files\n",
    "train.to_excel('../datasets/train.xlsx', index=False)\n",
    "test.to_excel('../datasets/test.xlsx', index=False)\n",
    "\n",
    "# Create a dictionary to store the degrons and their corresponding E3s in the train dataset\n",
    "# train_degron_E3 = {}\n",
    "# for index, row in train.iterrows():\n",
    "#     if row['degron_seq'] not in train_degron_E3:\n",
    "#         train_degron_E3[row['degron_seq']] = row['E3_sequence'].split(';')\n",
    "\n",
    "# # Save the train_degron_E3 dictionary to a csv file\n",
    "# with open('../datasets/train_degron_E3.csv', 'w') as f:\n",
    "#     for key in train_degron_E3.keys():\n",
    "#         f.write(\"%s,%s\\n\"%(key,train_degron_E3[key]))\n",
    "\n",
    "# # Create a dictionary to store the degrons and their corresponding E3s in the test dataset\n",
    "# test_degron_E3 = {}\n",
    "# for index, row in test.iterrows():\n",
    "#     if row['degron_seq'] not in test_degron_E3:\n",
    "#         test_degron_E3[row['degron_seq']] = row['E3_sequence'].split(';')\n",
    "\n",
    "# # Save the test_degron_E3 dictionary to a csv file\n",
    "# with open('../datasets/test_degron_E3.csv', 'w') as f:\n",
    "#     for key in test_degron_E3.keys():\n",
    "#         f.write(\"%s,%s\\n\"%(key,test_degron_E3[key]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  substrate      E3      degron_seq  \\\n",
      "0    Q16665  P00748  LAPAAGDTIISLDF   \n",
      "1    P17181  Q9Y297          DSGNYS   \n",
      "2    Q16665  P10523  LAPAAGDTIISLDF   \n",
      "3    P35659  Q969H0           ADSST   \n",
      "4    Q13002  Q6TDP4           APVIV   \n",
      "\n",
      "                                         E3_sequence  \n",
      "0  MRALLLLGFLLVSLESTLSIPPWEAPKEHKYKAEEHTVVLTVTGEP...  \n",
      "1  MDPAEAVLQEKALKFMCSMPRSLWLGCSSLADSMPSLRCLYNPGTG...  \n",
      "2  MAASGKTSKSEPNHVIFKKISRDKSVTIYLGNRDYIDHVSQVQPVD...  \n",
      "3  MNQELLSVGSKRRRTGGSLRGNPSSSQVDEEQMNRVVEEEQQQQLR...  \n",
      "4  MQPRSERPAGRTQSPEHGSPGPGPEAPPPPPPQPPAPEAERTRPRQ...  \n",
      "  substrate      E3 degron_seq  \\\n",
      "0    P46527  Q5XPI4   SVEQTPKK   \n",
      "1    Q14247  Q75N03      ADYRE   \n",
      "2    P14635  Q86WB0  PRTALGDIG   \n",
      "3    O00429  Q9NX47    RKRLPVT   \n",
      "4    P24864  O43791    LLTPPQS   \n",
      "\n",
      "                                         E3_sequence  \n",
      "0  MASKGAGMSFSRKSYRLTSDAEKSRVTGIVQEKLLNDYLNRIFSSS...  \n",
      "1  MDHTDNELQGTNSSGSLGGLDVRRRIPIKLISKQANKAKPAPRTQR...  \n",
      "2  MAAPCEGQAFAVGVEKNWGAVVRSPEGTPQKIRQLIDEGIAPEEGG...  \n",
      "3  MPDQALQQMLDRSCWVCFATDEDDRTAEWVRPCRCRGSTKWVHQAC...  \n",
      "4  MSRVPSPPPPAEMSSGPVAESWCYTQIKVVKFSYMWTINNFSFCRE...  \n"
     ]
    }
   ],
   "source": [
    "# Read train and test datasets\n",
    "train = pd.read_excel('../datasets/train.xlsx')\n",
    "test = pd.read_excel('../datasets/test.xlsx')\n",
    "\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n",
    "train_degrons = train[\"degron_seq\"].tolist()\n",
    "train_E3s = train[\"E3_sequence\"].tolist()\n",
    "\n",
    "test_degrons = test[\"degron_seq\"].tolist()\n",
    "test_E3s = test[\"E3_sequence\"].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: ProteinBERT: A Universal Deep-Learning Model of Protein Sequence and Function\n",
    "\n",
    "# All possible amino acids\n",
    "AAs = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "ADDITIONAL_TOKENS = ['<OTHER>', '<START>', '<END>', '<PAD>']\n",
    "\n",
    "# Each sequence is added <START> and <END> tokens\n",
    "ADDED_TOKENS_PER_SEQ = 2\n",
    "\n",
    "n_aas = len(AAs)\n",
    "aa_to_token_index = {aa: i for i, aa in enumerate(AAs)}\n",
    "additional_token_to_index = {token: i + n_aas for i, token in enumerate(ADDITIONAL_TOKENS)}\n",
    "token_to_index = {**aa_to_token_index, **additional_token_to_index}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "n_tokens = len(token_to_index)\n",
    "\n",
    "# Tokenize the degrons and E3s in the train and test datasets\n",
    "def tokenization(seq):\n",
    "    other_token_index = additional_token_to_index['<OTHER>']\n",
    "    return [additional_token_to_index['<START>']] + [aa_to_token_index.get(aa, other_token_index) for aa in parse_seq(seq)] + \\\n",
    "            [additional_token_to_index['<END>']]\n",
    "            \n",
    "def parse_seq(seq):\n",
    "    if isinstance(seq, str):\n",
    "        return seq\n",
    "    elif isinstance(seq, bytes):\n",
    "        return seq.decode('utf8')\n",
    "    else:\n",
    "        raise TypeError('Unexpected sequence type: %s' % type(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
