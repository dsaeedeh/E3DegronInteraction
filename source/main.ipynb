{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "1. E3-substrates-interactions file is downloaded from : http://ubibrowser.bio-it.cn/ubibrowser_v3/home/download\n",
    "2. Collected_degrons file is downloaded from: http://degron.phasep.pro/download/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Convert E3-substrates-interactions text file to Excel file'''\n",
    "# read in the text file using pandas\n",
    "df = pd.read_csv('../datasets/E3-substrates-interactions.txt', delimiter='\\t')\n",
    "\n",
    "# write the dataframe to an Excel file\n",
    "df.to_excel('../datasets/E3-substrates-interactions.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "1. Read the datasets\n",
    "2. Remove the rows with nan or invalid IDs on Entry column in degrons, and unreviewd on their Status column\n",
    "3. Remove the rows with nan or invalid IDs on both SwissProt AC (Substrate) and SwissProt AC (E3) columns\n",
    "4. Merge them together on Entry to a final dataset\n",
    "5. Keep needed columns: SwissProt AC (Substrate), SwissProt AC (E3), protein seq, start, end, deg_seq, E3_sequence\n",
    "6. Explode the rows containing # in their E3\n",
    "7. Add E3 sequences to the dataframe\n",
    "8. Groupby the final dataset to merge rows with the same substrate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the xlsx files using pandas\n",
    "degrons = pd.read_excel('../datasets/collected_degrons.xlsx')\n",
    "interactions = pd.read_excel('../datasets/E3-substrates-interactions.xlsx')\n",
    "E3_sequences = pd.read_excel('../datasets/uniprot-E3.xlsx')\n",
    "\n",
    "# Remove rows from both dataframes that have NaN or invalid ids\n",
    "degrons = degrons[(degrons['Entry'].notnull()) & (degrons['Entry'] != '-')]\n",
    "degrons = degrons[degrons['Status'] != 'unreviewd']\n",
    "\n",
    "# Remove the rows with nan or invalid IDs on both SwissProt AC (Substrate) and SwissProt AC (E3) columns\n",
    "interactions = interactions[(interactions['SwissProt AC (Substrate)'].notnull()) & (interactions['SwissProt AC (Substrate)'] != '-')]\n",
    "interactions = interactions[(interactions['SwissProt AC (E3)'].notnull()) & (interactions['SwissProt AC (E3)'] != '-')]\n",
    "\n",
    "# Merge them together on Entry to a final dataset\n",
    "final = pd.merge(degrons, interactions, left_on='Entry', right_on='SwissProt AC (Substrate)')\n",
    "final = pd.merge(final, E3_sequences, left_on='SwissProt AC (E3)', right_on='Entry')\n",
    "\n",
    "# Keep needed columns: SwissProt AC (Substrate), SwissProt AC (E3), protein seq, start, end, deg_seq\n",
    "final = final[['SwissProt AC (Substrate)', 'SwissProt AC (E3)', 'protein seq', 'start', 'end', 'deg_seq', 'E3_sequence']]\n",
    "final.columns = ['substrate', 'E3', 'substrate protein seq', 'start', 'end', 'degron_seq', 'E3_sequence']\n",
    "final = final.assign(E3=final['E3'].str.split('#')).explode('E3')\n",
    "final = final.drop_duplicates(subset=['substrate', 'E3'], keep='first')\n",
    "\n",
    "# Creat a map to store degrons and their corresponding substrates\n",
    "degron_substrate = {}\n",
    "for index, row in final.iterrows():\n",
    "    if row['degron_seq'] not in degron_substrate:\n",
    "        degron_substrate[row['degron_seq']] = row['substrate']\n",
    "        \n",
    "#Save the degron_substrate dictionary to a csv file\n",
    "with open('../datasets/degron_substrate.csv', 'w') as f:\n",
    "    for key in degron_substrate.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,degron_substrate[key]))\n",
    "\n",
    "# Creat a dictionary to store the E3s and their corresponding E3 sequences\n",
    "E3_sequence = {}\n",
    "for index, row in final.iterrows():\n",
    "    if row['E3'] not in E3_sequence:\n",
    "        E3_sequence[row['E3']] = row['E3_sequence']\n",
    "\n",
    "# Save the E3_sequence dictionary to a csv file\n",
    "with open('../datasets/E3_sequence.csv', 'w') as f:\n",
    "    for key in E3_sequence.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,E3_sequence[key]))\n",
    "\n",
    "# Groupby the final dataset to merge rows with the same substrate \n",
    "final = final.groupby(['substrate', 'substrate protein seq', 'start', 'end', 'degron_seq']).agg({'E3': lambda x: ';'.join(x), 'E3_sequence': lambda x: ';'.join(x)}).reset_index()\n",
    "\n",
    "# Write the final dataset to an Excel file\n",
    "final.to_excel('../datasets/final_dataset.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(29, 4)\n"
     ]
    }
   ],
   "source": [
    "# Test and train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(final, test_size=0.2, random_state=42)\n",
    "\n",
    "# Remove substrate protein seq, start, end columns from both train and test datasets\n",
    "train = train.drop(columns=['substrate protein seq', 'start', 'end'])\n",
    "test = test.drop(columns=['substrate protein seq', 'start', 'end'])\n",
    "\n",
    "#Print the size of the train and test datasets\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "# Write the train and test datasets to Excel files\n",
    "train.to_excel('../datasets/train.xlsx', index=False)\n",
    "test.to_excel('../datasets/test.xlsx', index=False)\n",
    "\n",
    "# Create a dictionary to store the degrons and their corresponding E3s in the train dataset\n",
    "train_degron_E3 = {}\n",
    "for index, row in train.iterrows():\n",
    "    if row['degron_seq'] not in train_degron_E3:\n",
    "        train_degron_E3[row['degron_seq']] = row['E3_sequence'].split(';')\n",
    "\n",
    "# Save the train_degron_E3 dictionary to a csv file\n",
    "with open('../datasets/train_degron_E3.csv', 'w') as f:\n",
    "    for key in train_degron_E3.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,train_degron_E3[key]))\n",
    "\n",
    "# Create a dictionary to store the degrons and their corresponding E3s in the test dataset\n",
    "test_degron_E3 = {}\n",
    "for index, row in test.iterrows():\n",
    "    if row['degron_seq'] not in test_degron_E3:\n",
    "        test_degron_E3[row['degron_seq']] = row['E3_sequence'].split(';')\n",
    "\n",
    "# Save the test_degron_E3 dictionary to a csv file\n",
    "with open('../datasets/test_degron_E3.csv', 'w') as f:\n",
    "    for key in test_degron_E3.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,test_degron_E3[key]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saeedehdavoudi/opt/anaconda3/envs/clean/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Adafactor, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"train_degron_E3.csv\", names=[\"degron\", \"E3\"])\n",
    "train_degrons = train_data[\"degron\"].tolist()\n",
    "train_E3s = train_data[\"E3\"].tolist()\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(\"test_degron_E3.csv\", names=[\"degron\", \"E3\"])\n",
    "test_degrons = test_data[\"degron\"].tolist()\n",
    "test_E3s = test_data[\"E3\"].tolist()\n",
    "\n",
    "# Initialize the tokenizer and the model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Tokenize the training and test data\n",
    "train_encodings = tokenizer(train_degrons, train_E3s, padding=True, truncation=True, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_degrons, test_E3s, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Create the dataset objects for PyTorch\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_encodings['labels'])\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_encodings['labels'])\n",
    "\n",
    "# Set training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=None,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
